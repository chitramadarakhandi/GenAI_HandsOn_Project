{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "!pip install -q transformers torch accelerate\n",
        "\n"
      ],
      "metadata": {
        "id": "GwWs2RwmmyQl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "zR9UInWW0S_A"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 1: TEXT **GENERATION**"
      ],
      "metadata": {
        "id": "EM9daTs-3i6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "text = \"Love is a beautiful\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "print(\"BERT does NOT support free text generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI-5AWM30a5H",
        "outputId": "6335d911-a692-4fd7-9903-19857d2b5b55"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT does NOT support free text generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa model\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
        "\n",
        "print(\"RoBERTa also does NOT support autoregressive generation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPAGfIT60hGU",
        "outputId": "0858c8cc-5108-4792-9688-608189f047d0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RoBERTa also does NOT support autoregressive generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 2: FILL-**MASK**"
      ],
      "metadata": {
        "id": "uOarFXwk3m4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BART model\n",
        "bart_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"facebook/bart-large\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "output = bart_generator(\n",
        "    \"Write a short poem about love:\",\n",
        "    max_length=80,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uUIPAmF0knA",
        "outputId": "7389cf96-33e6-4572-ad61-4a46790b0624"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a short poem about love: Erinlyo Bare Bareaida BareaidaEOeliaTE Bare Bare Bare Dome Bare Bare EE Dome Erin Bare BareTE Erin Bare Dome EE EE EE Bare DomeTETETEATERTEte BareTETE Erin EE EETETEtetete EE EEteteTEte EETEte ErinTEteTETEetteteterineTEte SweteteetteTEteetteteetteetteterineteteoneTEterineetteetteTErinerineTEerineTErineterinerineetteTEeetteetteetterinerinerineteetteeetterineeetteerineetterineibeletteetteleeetterineetteeleeetteetteoetteetteeuribeletteibeletterineTEetterineleeibeletteeuretteetteeeureuretteibelTEibelette FlameetteibeleuretteeurleeibelTEetteleeibeleuribeliffsTEibelTEleeibeliffseuribelTErineibeleurleeleeeuribel Bettyibeletteleeuribeleur FlameibeletteillyibeletteleeTEleeleeibel FlameibelTEeuribelrineette FlameibeleureurleeeureureuribelleeibeloneetteetteibelleeleeTEibeleur MelleureurrineibelTEleibeleuriffseureur Flameeureuriffsiffseurleeuriffsleeibel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating poems on love,coding and space **bold text**"
      ],
      "metadata": {
        "id": "dZDxRJkh5wCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2-medium\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_jDZWs55cFc",
        "outputId": "266ee4e1-a6f5-4b84-91f3-6c7497fbad64"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(theme, mode=\"poem\"):\n",
        "    if mode == \"poem\":\n",
        "        return f\"\"\"\n",
        "Write a short rhyming poem with 3 stanzas about the theme \"{theme}\".\n",
        "Each stanza should have 2 lines.\n",
        "Poem:\n",
        "\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"\n",
        "Write song lyrics with a chorus about the theme \"{theme}\".\n",
        "Make it emotional and rhythmic.\n",
        "Song:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Lfjj_ksq5d78"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(theme, mode=\"poem\"):\n",
        "    prompt = build_prompt(theme, mode)\n",
        "\n",
        "    output = generator(\n",
        "        prompt,\n",
        "        max_length=180,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "    return output[0][\"generated_text\"]\n"
      ],
      "metadata": {
        "id": "LXJrLdbc5i7X"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"Love\", mode=\"poem\"))\n",
        "# print(generate_text(\"Space\", mode=\"song\"))\n",
        "# print(generate_text(\"Coding\", mode=\"poem\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XniTlFK-5mUc",
        "outputId": "43aebc4c-eb64-47ab-deaa-336a20651d78"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=180) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Write a short rhyming poem with 3 stanzas about the theme \"Love\".\n",
            "Each stanza should have 2 lines.\n",
            "Poem:\n",
            "\"Hmmm, what's the story?\n",
            "When I was your age, I was so busy\n",
            "A real man should have been a real man\n",
            "When I was your age, I thought I was a real man\n",
            "\"\n",
            "Hmmm, what's the story?\n",
            "When I was your age, I was so busy\n",
            "(What a difference a day makes)\n",
            "When I was your age, I thought I was a real man\n",
            "(Hmmmm)\n",
            "Poem:\n",
            "\"Love is just love.\n",
            "Love is just love.\"\n",
            "Hmmm, what's the story?\n",
            "When I was your age, I thought I was a real man\n",
            "When I was your age, I thought I was a real man(Hmmm)\n",
            "Hmmmm, what's the story?\n",
            "When I was your age, I thought I was a real man(Hmmm)\n",
            "\"Love is just love.\"\n",
            "Poem:\n",
            "\"And so I thought, \"What do you know about love?\"\n",
            "\"And so I thought, \"What do you know about love?\"\n",
            "\"What do you know about love?\"\n",
            "\"Love is just love.\n",
            "Hmmm, what's the story?\"\n",
            "\"When I was your age, I thought I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2-medium\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "output = gpt2_generator(\n",
        "    \"Write a rhyming poem about coding:\",\n",
        "    max_length=120,\n",
        "    do_sample=True,\n",
        "    temperature=0.9\n",
        ")\n",
        "\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZX5lpmN1Ggt",
        "outputId": "9f4822cb-46f3-4e80-a284-35c3ebb106e7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write a rhyming poem about coding: The only thing you have to do is, say, 'I learned to code!' And this is going to be something you really want to be learning. I wanted to write this poem about learning to code.\n",
            "\n",
            "For an hour, you write a poem. You can go through this as many times as you want, and you're going to get a lot of useful knowledge. You only have to remember half of it, but you are going to be able to go through it, learn it, and think of it later. You're going to write the words, and you're going to write the whole poem. So, the good thing about a poem is, if you think about something, you can't be distracted. You'll never forget everything I wrote, but you'll be able to go through it and think about it later. If you like poetry, then it's very useful.\n",
            "\n",
            "To sum up: It can only take you three hours to learn the basics of programming.\n",
            "\n",
            "If you've heard this type of advice before, you know that you should pick up a programming program if you want to improve your programming skills. You may think, \"I need to learn how to fix this bug.\" Or you may think, \"I need to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model\n",
        "bert_fill = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "print(bert_fill(\"AI can [MASK] poetry.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNYyMoZK1lKx",
        "outputId": "9cd8e970-e449-4c67-f209-7c0309797064"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.44586440920829773, 'token': 4339, 'token_str': 'write', 'sequence': 'ai can write poetry.'}, {'score': 0.14341701567173004, 'token': 3191, 'token_str': 'read', 'sequence': 'ai can read poetry.'}, {'score': 0.055543385446071625, 'token': 3443, 'token_str': 'create', 'sequence': 'ai can create poetry.'}, {'score': 0.053389985114336014, 'token': 2079, 'token_str': 'do', 'sequence': 'ai can do poetry.'}, {'score': 0.03861717879772186, 'token': 2191, 'token_str': 'make', 'sequence': 'ai can make poetry.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa model\n",
        "roberta_fill = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"roberta-base\"\n",
        ")\n",
        "\n",
        "print(roberta_fill(\"AI can <mask> poetry.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ8lgXzU1prD",
        "outputId": "4d2aef11-46cc-42f4-9e4c-019570137510"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.29186174273490906, 'token': 3116, 'token_str': ' write', 'sequence': 'AI can write poetry.'}, {'score': 0.23586641252040863, 'token': 1166, 'token_str': ' read', 'sequence': 'AI can read poetry.'}, {'score': 0.05403473600745201, 'token': 146, 'token_str': ' make', 'sequence': 'AI can make poetry.'}, {'score': 0.04838563874363899, 'token': 1346, 'token_str': ' understand', 'sequence': 'AI can understand poetry.'}, {'score': 0.03410880267620087, 'token': 1045, 'token_str': ' create', 'sequence': 'AI can create poetry.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BART model\n",
        "bart_fill = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"facebook/bart-large\"\n",
        ")\n",
        "\n",
        "print(bart_fill(\"AI can <mask> poetry.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy0Ro7Ms1tmR",
        "outputId": "6a9271af-e5d9-4699-b2b2-f12ae90d7e9e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.374755859375, 'token': 9115, 'token_str': ' refer', 'sequence': 'AI can refer poetry.'}, {'score': 0.185546875, 'token': 28, 'token_str': ' be', 'sequence': 'AI can be poetry.'}, {'score': 0.1256103515625, 'token': 1266, 'token_str': ' mean', 'sequence': 'AI can mean poetry.'}, {'score': 0.1160888671875, 'token': 67, 'token_str': ' also', 'sequence': 'AI can also poetry.'}, {'score': 0.020172119140625, 'token': 14620, 'token_str': ' translate', 'sequence': 'AI can translate poetry.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK 3: QUESTION ANSWERING"
      ],
      "metadata": {
        "id": "WRnJ2qkA3t6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "GPT-2 is a decoder-only transformer model.\n",
        "It is widely used for text generation tasks such as poetry and storytelling.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What is GPT-2 used for?\"\n"
      ],
      "metadata": {
        "id": "0JbYk_EY2B7n"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT model\n",
        "bert_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"bert-base-uncased\"\n",
        ")\n",
        "\n",
        "print(bert_qa(question=question, context=context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTzhax_Q1ySR",
        "outputId": "eda3ce03-29d5-4152-d91f-a73d4c0d8917"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.0025132859591394663, 'start': 7, 'end': 36, 'answer': 'is a decoder-only transformer'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa model\n",
        "roberta_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"deepset/roberta-base-squad2\"\n",
        ")\n",
        "\n",
        "print(roberta_qa(question=question, context=context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80HFjnFz2JN0",
        "outputId": "1ce75f27-d7bd-4257-e0e0-5beec87dba6e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.43160074949264526, 'start': 66, 'end': 87, 'answer': 'text generation tasks'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BART model\n",
        "bart_qa = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")\n",
        "\n",
        "print(bart_qa(question=question, context=context))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmUo7F1A2Pro",
        "outputId": "cc6ecbcc-62e0-4726-dd06-6eacf2780e4f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.0024642166681587696, 'start': 19, 'end': 24, 'answer': '-only'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task        | Model     | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "|-------------|-----------|----------------------------------|----------------------------------------|---------------------------------------------|\n",
        "| Generation  | BERT      | Failure | Failed to generate continuous or meaningful text. | BERT is an encoder-only model and does not support autoregressive generation. |\n",
        "| Generation  | RoBERTa   | Failure | Produced incoherent or unusable output for text generation. | RoBERTa is also encoder-only and not trained for next-token prediction. |\n",
        "| Generation  | BART      | Success | Generated coherent and context-aware text. | BART uses an encoderâ€“decoder architecture designed for generative tasks. |\n",
        "| Generation  | GPT-2     | Success | Generated fluent, creative, and rhyming poems based on the given theme. | GPT-2 is a decoder-only autoregressive model trained for text generation. |\n",
        "| Fill-Mask   | BERT      | Success | Correctly predicted masked words in sentences. | BERT is trained using Masked Language Modeling (MLM). |\n",
        "| Fill-Mask   | RoBERTa   | Success | Predicted accurate and contextually relevant masked tokens. | RoBERTa improves MLM training using dynamic masking and larger datasets. |\n",
        "| Fill-Mask   | BART      | Success | Successfully filled masked tokens with fluent language. | BART learns denoising autoencoding, enabling masked token recovery. |\n",
        "| QA          | BERT      | Success | Extracted correct answers from the given context. | BERT is well-suited for extractive question answering tasks. |\n",
        "| QA          | RoBERTa   | Success | Provided more precise answers compared to BERT. | RoBERTa has stronger contextual representations due to improved training. |\n",
        "| QA          | BART      | Success | Generated correct answers but occasionally added extra text. | BART is generative and may produce abstractive-style responses. |\n"
      ],
      "metadata": {
        "id": "CtAcIpq22umv"
      }
    }
  ]
}